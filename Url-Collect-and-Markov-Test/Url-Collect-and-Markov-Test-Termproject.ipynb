{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Test\n",
    "## CS 521 Term Project\n",
    "Qiren Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. This project collected the random webiste and uesd the 2-grams model to obtain the high frequency vocabulary. An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n − 1)-order Markov model. Both n-gram model and morkov chain model are uesful in NLP(natural language processing). This project choosed the highest frequecy vocabulary that occurred in the 2-grams model to take the markov test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Url Collect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: Collecting random website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class urlCollect:\n",
    "    def __init__(self,internal=[],bsObj=0,addressParts=0):\n",
    "        self.internal=internal\n",
    "        self.bsObj=bsObj\n",
    "        self.addressParts=addressParts\n",
    "    \n",
    "    def getIn(self,bsObj,inUrl):    \n",
    "        self.internal=[]       \n",
    "        for i in self.bsObj.findAll(\"a\",href=re.compile(\"^\\/wiki\")):\n",
    "            \n",
    "            if i.attrs['href'] is not None:\n",
    "                if i.attrs['href'] not in self.internal:\n",
    "                    self.internal.append('https://en.wikipedia.org'+i.attrs['href'])\n",
    "        return self.internal\n",
    "    \n",
    "    def splitAddress(self,address):\n",
    "        self.addressParts=address.replace(\"http://\",\"\").split('/')\n",
    "        return self.addressParts\n",
    "        \n",
    "    def getRanEx(self,startingPage):    \n",
    "        headers={'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "             'Accept-Encoding': 'gzip, deflate, br',\n",
    "             'Accept-Language': 'zh,en-US;q=0.9,en;q=0.8,zh-TW;q=0.7',\n",
    "             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n",
    "        html=requests.get(startingPage,headers=headers)   \n",
    "        self.bsObj=BeautifulSoup(html.text,\"lxml\")   \n",
    "        self.internal=self.getIn(self.bsObj,self.splitAddress(startingPage)[0])    \n",
    "        return self.internal[random.randint(0,len(self.internal)-1)]\n",
    "       \n",
    "    def fllowEx(self,startingSite):\n",
    "        self.internal=self.getRanEx(startingSite)\n",
    "        print(\"Casual：\"+self.internal+'\\n')\n",
    "        return self.internal     \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clean Data, Build ngrams model and Generate Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordListSum(wordList):\n",
    "    sum = 0\n",
    "    for word, value in wordList.items():\n",
    "        sum += value\n",
    "    return sum\n",
    "\n",
    "def retrieveRandomWord(wordList):\n",
    "    randIndex = randint(1, wordListSum(wordList))\n",
    "    for word, value in wordList.items():\n",
    "        randIndex -= value\n",
    "        if randIndex <= 0:\n",
    "            return word\n",
    "\n",
    "def buildWordDict(text):\n",
    "    text = text.replace(\"\\n\", \" \");\n",
    "    text = text.replace(\"\\\"\", \"\");\n",
    "    punctuation = [',', '.', ';',':']\n",
    "    for symbol in punctuation:\n",
    "        text = text.replace(symbol, \" \"+symbol+\" \");\n",
    "    words = text.split(\" \")\n",
    "    words = [word for word in words if word != \"\"]\n",
    "    wordDict = {}\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i-1] not in wordDict:\n",
    "            wordDict[words[i-1]] = {}\n",
    "        if words[i] not in wordDict[words[i-1]]:\n",
    "            wordDict[words[i-1]][words[i]] = 0\n",
    "        wordDict[words[i-1]][words[i]] = wordDict[words[i-1]][words[i]] + 1\n",
    "    return wordDict\n",
    "\n",
    "def cleanInput(input):\n",
    "    input = re.sub('\\n+', \" \", input)\n",
    "    input = re.sub('\\[[0-9]*\\]', \"\", input)\n",
    "    input=re.sub('\\t+',\" \",input)\n",
    "    input = re.sub(' +', \" \", input)\n",
    "    input=isCommon(input)\n",
    "    input = bytes(input, \"utf-8\")\n",
    "    input = input.decode(\"ascii\", \"ignore\")    \n",
    "    cleanInput = []\n",
    "    input = input.split(' ')\n",
    "    for item in input:\n",
    "        item = item.strip(string.punctuation)\n",
    "        if len(item) > 1 or (item.lower() == 'a' or item.lower() == 'i'):\n",
    "            cleanInput.append(item)\n",
    "    cleanInput=' '.join(cleanInput)\n",
    "    return cleanInput\n",
    "\n",
    "def ngrams(input, n):\n",
    "    #input=cleanInput(input)\n",
    "    output={}\n",
    "    for i in range(len(input)-n+1):\n",
    "        ngramTemp = \" \".join(input[i:i+n])\n",
    "        if ngramTemp not in output:\n",
    "            output[ngramTemp] = 0\n",
    "        output[ngramTemp] += 1\n",
    "    return output\n",
    "\n",
    "def isCommon(content):\n",
    "    b=[]\n",
    "    commonWords = [\"the\", \"be\", \"and\", \"of\", \"a\", \"in\", \"to\", \"have\", \"it\",\n",
    "    \"i\", \"that\", \"for\", \"you\", \"he\", \"with\", \"on\", \"do\", \"say\", \"this\",\n",
    "    \"they\", \"is\", \"an\", \"at\", \"but\",\"we\", \"his\", \"from\", \"that\", \"not\",\n",
    "    \"by\", \"she\", \"or\", \"as\", \"what\", \"go\", \"their\",\"can\", \"who\", \"get\",\n",
    "    \"if\", \"would\", \"her\", \"all\", \"my\", \"make\", \"about\", \"know\", \"will\",\n",
    "    \"as\", \"up\", \"one\", \"time\", \"has\", \"been\", \"there\", \"year\", \"so\",\n",
    "    \"think\", \"when\", \"which\", \"them\", \"some\", \"me\", \"people\", \"take\",\n",
    "    \"out\", \"into\", \"just\", \"see\", \"him\", \"your\", \"come\", \"could\", \"now\",\n",
    "    \"than\", \"like\", \"other\", \"how\", \"then\", \"its\", \"our\", \"two\", \"more\",\n",
    "    \"these\", \"want\", \"way\", \"look\", \"first\", \"also\", \"new\", \"because\",\n",
    "    \"day\", \"more\", \"use\", \"no\", \"man\", \"find\", \"here\", \"thing\", \"give\",\n",
    "    \"many\", \"well\"]\n",
    "    for word in content.split(' '):\n",
    "        if word not in commonWords:\n",
    "            b.append(word)\n",
    "    b=' '.join(b)\n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Invoke Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casual：https://en.wikipedia.org/wiki/North_America\n",
      "\n",
      "['North America', 'United States', 'North American', 'Archived original', 'June 2011', 'Central American', 'Central America', 'South America', 'Free Trade', 'Trade Agreement'] \n",
      "\n",
      "North America Alternatively geologists physiographically locate southern limit Isthmus Panama Canal Figures listed are common region had 1867 already conducted largest trade blocs Canada-Costa Rica 88 , 720 Liga MX Football soccer compete comes big business Guardian . 026 pop . mw-parser-output q{quotes : 100%} . 7 million 1 , 320 2018 North American Region PDF November 2007 What origin Central Intelligence Agence Retrieved June 2011 Garcia-Castellanos Lombardo 2007 Retrieved 20 agriculture comprises 77 population density PDF Press release Bureau definitions Aruba Curaao Sint Eustatius Netherlands Antilles Leeward Antilles Archived original 21 teams 2017 Population estimates are suitable people whom \n"
     ]
    }
   ],
   "source": [
    "ages=set()\n",
    "random.seed(datetime.datetime.now())    \n",
    "#web collect\n",
    "url=urlCollect().fllowEx('https://en.wikipedia.org/wiki/Come_and_Get_Your_Love')\n",
    "headers={'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "         'Accept-Encoding': 'gzip, deflate, br',\n",
    "         'Accept-Language': 'zh,en-US;q=0.9,en;q=0.8,zh-TW;q=0.7',\n",
    "         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n",
    "html=requests.get(url,headers=headers)   \n",
    "bsObj=BeautifulSoup(html.text,\"lxml\")   \n",
    "content=bsObj.find(\"div\",{\"id\":\"mw-content-text\"}).get_text()\n",
    "\n",
    "#clean content\n",
    "content = cleanInput(content)\n",
    "contents=content.split(' ')\n",
    "ngrams = ngrams(contents, 2)\n",
    "sortedNGrams = OrderedDict(sorted(ngrams.items(), key=lambda t: t[1], reverse=True))\n",
    "abstract=[]\n",
    "for i in range(10):\n",
    "    abstract.append(list(sortedNGrams.keys())[i])\n",
    "print(abstract,'\\n')\n",
    "wordDict = buildWordDict(content)\n",
    "\n",
    "#generate chain length is 100 markov chain\n",
    "length = 100\n",
    "chain = \"\"\n",
    "currentWord = abstract[0].split(' ')[0]\n",
    "for i in range(0, length):\n",
    "    chain += currentWord+\" \"\n",
    "    currentWord = retrieveRandomWord(wordDict[currentWord])\n",
    "print(chain)\n",
    "\n",
    "#save result to txt file \n",
    "with open('markex.txt',\"w\") as f:\n",
    "    x=url+'\\n'+chain\n",
    "    f.write(x)    \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This project does not explore the Markov chain model and n-gram model's power in real life and I will continue to complete my work at github. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
